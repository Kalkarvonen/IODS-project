---
title: "Clustering and classification"
author: "Kalle Karvonen"
date: "2023-11-27"
output: html_document
---
# Clustering and Classification

- Loading in the Boston dataset, which contains the housing values in suburbs of Boston in 506 rows of data and 14 different columns of variables such as crime rate median value,age and proportion of blacks by town.
```{r}
library(MASS)
library(ggplot2)

dim(Boston)
str(Boston)
```
-Here we can summarize the variables on the data to see a bit how the values line up. We can also perform a pairwise analysis on the data to visualize it and the different variables effect on others. The data seems to have a high variability between datas and no clear distributions can be clearly seen by eye
```{r}
summary(Boston)
pairs(Boston)
```
- This data above can be pretty hard to interpret due to it having so large amount of plots so we could for example do this to see how the plots look for three variables such as  the amount of crime/capita, amount of blacks in the neighbourhood and the median property value. 

-Based on this we could make some interpetations such as that the crime rate correlates negatively with the median value and that there seems to be no clear visual indication that the amount of blacks would correlate with the amount of crime or median value of owner occupied homes.

```{r}
pairs(Boston[,c(1,12,14)])
```
-Another good way to show any correlations on the data would be to use the correlation matrix which can be used to show correlation between variables
```{r}
# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) 

# print the correlation matrix
print(cor_matrix)

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle")
```
- there are some variables that seem to correlate with each other.


- This dataset could use some scaling to make the distributions of the data a bit more readable. so lets perform scaling

```{r}
# center and standardize variables by using the mean as the center point
boston_scaled <- as.data.frame(scale(Boston))

# summaries of the variables
summary(Boston)
summary(boston_scaled)

```
- now we see that the values are scaled so that all the dataframes show identical mean of zero meaning they have been centered around it.Basically this means we now have some negative values but overall we should have less outliers.

-Now we can add categorical variables to the crime rates of our scaled data by first doing quantiles of values and then labeling these quantiles. 
```{r}
# c reating the quantile bins for our labeling
bins <- quantile(boston_scaled$crim)
bins
#creating a vector label which contains our desired labels
label <- c("low","med_low","med_high","high")
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE,labels = label)

# look at the table of the new factor crime
table(crime)
# now we have a uniform distribution of crime rate factors
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

- Now we can use the created scaled data with categorical valuables to create training and test data for labeling purposes by dividing our dataset to 80% 20% split between training and test sets
- we also save the correct classes from our test data for quality control later on and remove the variable crime from the test data
-So we use 80% of the data to train our model and 20% of data to do a test to see how accurate our model is.
```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data so that we can use our model to label it
test <- dplyr::select(test, -crime)
```


- Now we will perform a linear discriminant analysis to perform dimensionality reduction of our data before classification. This analysis is similar to Principal component analysis(PCA) in that it also searches for combinations of variables that explain our data .

- we perform it so that we use the crime variable we created as our target variable and test it against other variables in the dataset as predictor variables

```{r}
# linear discriminant analysis for 
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results (select both lines and execute them at the same time!)
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 1)
```
- So by looking at the proportion of trace in the summary we can see that 96% of variance between our data can be explained by the first discriminant which means that LD1 is good in distinguishing between classes

- Now we can save the crime categories from the test set and predict classes with our LDA model on the test data. We previously saved the correct classes so we can use those to validate our model

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```
- The cross table of our model shows that our model predicts high crime rates correctly. However for the other classes(low,med_low and med_high) it only predicts 65%,42% and 68% correctly so our model should be tweaked to get better accuracy. Perhaps different classifiers might work better than using lda.


- Here we reload our data and scale it like before just so that we have the correct data for K-means algorithm
```{r}
data("Boston")
boston_scaled <- as.data.frame(scale(Boston))

#distance calculations
# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled,method = "manhattan")

# look at the summary of the distances
summary(dist_man)
```
-Euclidean distance seems to generally be shorter in this data since it is just a straight line between data points. 

- now we can run K-means using our data
```{r}
set.seed(6)

# determine the number of clusters
k_max <- 5

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <- kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
```
- Based on our twcss we can say that the optimal number of clusters is 2 as that is where the value starts dropping drastically, but 3 could also be a valid option to test.

as for the pairs analysis now we see which of the 2 clusters our data labels itself into and we can see that there are a lot of data where one cluster is overrepresented showing that the data lines up with it more.

```{r}
date()
```


